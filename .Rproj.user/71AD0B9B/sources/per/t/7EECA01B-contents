---
output: html_document
editor_options: 
  chunk_output_type: console
---
# Muestreo y estimaciones

## Conceptos 

  - **Población (universo)** ($U$): La colección completa de los elementos de interés.
  - **Muestra** ($s$): Un subcobjunto de la población de interés o el universo.
  
$$s \subset U$$

  - **Muestreo**: El mecanismo utilizado para extraer la muestra del universo, en la estadística se valora principalmente a un **mecanismo aleatorio**, esto debido a las propiedades que este genera (*Muestra valida estadísticamente*).
  - **Inferencia**: Es un área de la estadística que busca a partir de una muestra estadísticamente valida *describir* a la población de estudio. Mediante la inferencia descriptiva, inferencia predictiva y la inferencia causal. 
  - **Parámetro**: Una función sobre la población
  
$$\theta=f(U,X)$$

  - **Estadística:** Una función sobre la muestra
  
$$Estadística=f(s,X)$$

  - **Estimador**: Es una estadística, que tiene el objetivo de aproximar a un parámetro
  
$$\hat{\theta}=f(s,X)$$
  
Ejemplo.

En una población de estudiantes de tamaño $N=10$, se tiene el gasto en transporte a la universidad en un día común. Se extrae una muestra de tamaño 4, proponer estimadores para el total del gasto en transporte.


```{r}
x<-c(2,4,5,5,2,10,15,7,4,2)
sum(x)#parámetro del total

s<-sample(x,4)
s<-c(5,5,2,7)

s2<-sample(x,4)
s2<-c(10,5,5,15)
```

$$\hat{\theta}_1=\sum_s x_i+n*N =5+5+2+7+40=\{59,75\}$$

$$\hat{\theta}_2=n\sum_{s,<>}x_i=(5+2+7)*4=\{56,120\}$$

$$\hat{\theta}_3=\frac{1}{x_{max}}\prod_s x_i= \frac{5*5*2*7}{7}=\{50,250\}$$
$$\hat{\theta}_4=\frac{N\sum_s x_i}{n}=N*\bar{x}=\frac{10*(5+5+2+7)}{4}=\{47.5,87.5\}$$

> ¿Cuántas muestras posibles se pueden construir en el ejercicio anterior?

Si las muestras son sin reposición la cantidad de muestras posibles es determinada por:

$$NCn=10C4=\frac{10!}{6!*4!}=210$$

```{r}
combn(x,4)
choose(10,4)
format(choose(800,20),scientific = F)
```

## Distribuciones muestrales

Es la forma o el comportamiento de las muestras posibles respecto algún estimador, su importancia se debe a la posibilidad de estudiar el fenómeno completo e identificar patrones.

```{r}
ss<-combn(x,4)
t1<-apply(ss,2,sum)+4*10
t2<-sapply(apply(ss,2,unique), sum)*4
t3<-apply(ss,2,prod)/apply(ss,2,max)
t4<-apply(ss,2,mean)*10
tt<-cbind(t1,t2,t3,t4)
#representante
apply(tt,2,mean)
#variabilidad
apply(tt,2,sd)
#gráfica
par(mfrow=c(2,2))
hist(t1,xlim=c(0,100));hist(t2,xlim=c(0,100));hist(t3,xlim=c(0,100));hist(t4,xlim=c(0,100))
boxplot(t1);boxplot(t2);boxplot(t3);boxplot(t4)
plot(density(t1));abline(v=56);plot(density(t2));abline(v=56);plot(density(t3));abline(v=56);plot(density(t4));abline(v=56)
```

Los estimadores $\hat{\theta}$ finalmente son variables aleatorias ya que a priori no sabemos el resultado antes de sacar la muestra respectiva. Cuyo recorrido de esta variable aleatoria es dada por los distintos valores que obtienen de las muestras posibles. 

Existen dos criterios prácticos para calificar la calidad de un estimador:

  * Estimador insesgado (más relevante): Se refiere a que el centro de la distribución es el parámetro de interés, de manera formal:

$$E[\hat{\theta}]=\theta$$  
  
  * Estimador eficiente: La idea de eficiencia en los estimadores se mide con la varianza del estimador, el estimador con menor varianza es el más eficiente. Por ejemplo:

  
$$V(\hat{\theta}_1)>V(\hat{\theta}_2)$$  
  
$\hat{\theta}_2$ es más eficiente que $\hat{\theta}_1$

Existen estimadores clásicos y frecuentemente usados que son estimadores insesgados, entre ellos:

### Media

  * Parámetro
  
$$\mu_x=\frac{\sum_U x_i}{N}$$

  * Estimador (media muestral)

$$\bar{x}=\frac{\sum_s x_i}{n}$$

### Diferencia de medias

Es una medida que compara dos poblaciones

  * Parámetro
  
$$\mu_1-\mu_2=\frac{\sum_{U_1} x_i}{N_1}-\frac{\sum_{U_2} x_i}{N_2}$$

  * Estimador (medias muestrales)
  
$$\bar{x}_1-\bar{x}_2=\frac{\sum_{s_1} x_i}{n_1}-\frac{\sum_{s_2} x_i}{n_2}$$
### Total

  * Parámetro
  
$$t_x=\sum_U x_i$$

  * Estimador 

$$\hat{t}_x=N*\frac{\sum_s x_i}{n}=N*\bar{x}$$

### Proporción 

Es una medida que identifica la proporción de participación de alguna característica de interés


  * Parámetro
  
$$P_A=\frac{\#A}{N}=\frac{\sum_U x_i}{N}; \quad \{x_i=1 \quad i \in A, x_i=0 \quad  eoc\}$$

  * Estimador
  
$$\hat{P}_A=\frac{\#a}{n}=\frac{\sum_s x_i}{n}; \quad \{x_i=1 \quad i \in A, x_i=0 \quad  eoc\}$$

## Teorema del limite central

Si $\bar{x}$ es la media muestral de una muestra aleatoria de tamaño $n$ tomada de una población $U$ con media poblacional $\mu_x$ y varianza finita (se puede calcular)  $\sigma_x^2$. Entonces la forma limite de la **distribución** de $\bar{x}$ a medida que $n\rightarrow \infty$ **crece**, se puede asegurar:

$$\bar{x}\sim N\left(\mu=\mu_x,\sigma^2=\frac{\sigma^2_x}{n}\right)$$

Nota: Esta idea de $n$ grande, en estadística tradicionalmente se toma como "grande" cuando $n\geq 30$, hay textos que plantean $n\geq 20$.

Ejemplo:

Recordar la idea de una distribución muestral:

```{r}
x<-c(2,4,5,5,2,10,15,7,4,2)
n<-4
choose(10,4)
ss<-combn(x,4)
ss
#distribución de la media
mm<-apply(ss,2,mean)
mm
hist(mm)
mean(x) #parámetro media poblacional
mean(mm) # Insesgadez
```

Vamos a **simular** el teorema del limite central.

Imaginar que se tiene una población de 500 gatos y se quiere observar el peso en kilogramos.

```{r}
N<-500
set.seed(855)
xx<-rnorm(N, 4,1.2)
xx<-round(xx,1)
xx
n<-40
format(choose(N,n),scientific = F)# muestras posibles
```

Como no es posible estudiar a todas las muestras posibles, debido a una limitación computacional se va a obtener y estudiar una parte de estas distribuciones muestrales.

```{r}
k<-1000 # parte de las muestras posibles
mm<-NULL
for(i in 1:k){
  maux<-sample(xx,n)
  mm[i]<-mean(maux)  
}
mm
hist(mm,xlim = c(3,5))
abline(v=mean(xx),col="red",lwd=2)

vx<-sum((xx-mean(xx))^2)/N
# la distribución teórica
curve(dnorm(x,mean(xx),sqrt(vx/n)),xlim = c(3,5))

# de forma gráfica
hist(mm,xlim = c(3,5),freq = F)
curve(dnorm(x,mean(xx),sqrt(vx/n)),add=T,col="red",lwd=2)
```

> Nota: El teorema del limite central supone poblaciones grandes ("infinitas")

> Nota: Cuando las poblaciones son pequeñas el teorema del limite central se puede seguir usando, sin embargo, la convergencia a la normalidad no estan fina

## Distribución muestral de la media para muestras de 30 o más

Sea $\bar{x}$ la media muestral un estimador para la media poblacional $\mu_x$.

A partir del teorema de limite central y tomando en cuenta $n\geq 30$. Se puede definir a su distribución muestral como:

$$\bar{X} \sim N\left(\mu_x,\frac{\sigma_x^2}{n}\right)\approx N\left(\bar{x},\frac{s^2}{n} \right)$$

De manera general para la distribución de la media muestral podemos mencionar lo siguiente:

  * Es un estimador insesgado, de tal forma que $E[\bar{x}]=\mu$
  * La variabilidad de la media muestra es:
  
$$V(\bar{x})=\sigma^2_{\bar{x}}=\frac{\sigma^2}{n}$$  

  * La estimación de la varianza viene dada por:

$$s^2_{\bar{x}}=\frac{s^2}{n}$$
  
Este último resultado también se denomina el error estándar del estimador de la media cuando:

$$s_{\bar{x}}=\sqrt{\frac{s^2}{n}}=EE(\bar{x})$$

> Ejercicio 1: 

Se tiene el dato del peso medido en kg. de una muestra de 9 animales:

4.0, 4.3, 4.5, 4.6, 4.7, 4.8, 4.9, 4.9, 5.1.

Obtener la media muestral y el error estándar.

Solución:

$$\bar{x}=\frac{\sum_s x_i}{n}=\frac{41.8}{9}=4.64$$

Para el error estándar:

$$s^2=\frac{\sum_s (x_i-\bar{x})^2}{n-1}=0.1153$$

La varianza del estimador:

$$s^2_{\bar{x}}=\frac{0.1153}{9}=0.0128$$

Finalmente el error estándar es:

$$s_{\bar{x}}=\sqrt{0.0128}=0.1132$$

## Intervalo de confianza, límites de confianza

Una segunda forma de aproximarse al valor real de un parámetro poblacional, es mediante un intervalo de confianza que brinda limites en los que se encuentra el verdadero valor del parámetro a un nivel determinado de confiabilidad. En términos de probabilidades, el objetivo es determinar:

$$P(L<\theta<U)=1-\alpha=\text{Nivel de confiabilidad}$$
El valor $\alpha$ se conoce como la significancia y sus valores más usuales son de $0.01, 0.05,0.1$, que respectivamente representan el 99%, 95% y 90% de confiabilidad.

El objetivo ahora es determinar la forma de los límites $L=Lower$ y $U=Upper$, esto depende del estimador con el que se trabaje, para el estimador de la media, si usamos el teorema del límite central sabemos que:

$$\bar{X} \sim N\left(\mu_x,\frac{\sigma_x^2}{n}\right)$$
Ahora, si realizamos una transformación:

$$Z=\frac{\bar{X}-\mu_x}{\frac{\sigma_x}{\sqrt{n}}}\sim N(0,1)$$
Por ejemplo, para un $\alpha=0.05$

$$P(-z_{0.05/2}<Z<z_{0.05/2})=0.95 \quad(95\%)$$

De esta forma, el intervalo de confianza de la media es:

$$IC_{1-\alpha}(\mu): \bar{x} \pm z_{\alpha/2} *\frac{S}{\sqrt{n}}$$
> Nota: 

Esta formula se aplica usando las información de la muestra, en algunos casos es posible contar con la varianza original de los datos $\sigma^2$, si esto sucede la formula es:

$$IC_{1-\alpha}(\mu): \bar{x} \pm z_{\alpha/2} *\frac{\sigma}{\sqrt{n}}$$

Los valores más comunes para confiabilidad son:

  * 99% de confiabilidad: $z_{\alpha/2}=2.58$
  * 95% de confiabilidad: $z_{\alpha/2}=1.96$
  * 90% de confiabilidad: $z_{\alpha/2}=1.64$

> Ejercicio: (Capitulo 6, ejericio 6.)

El tiempo de incubación de huevos de lagarto fue medido para 24 lagartos. Estos 24 lagartos provienen de una población que tiene un 
$$\sigma^2=89.06 días^2$$
Y la media muestral
$$\bar{x}=61.4 días$$

  * Calcular el IC al 99% para la media de incubación
  * Calcular el IC al 95% para la media de incubación
  * Calcular el IC al 90% para la media de incubación

Solución

Al 99%,

$$IC_{1-\alpha}(\mu): 61.4 \pm 2.58 *\sqrt{\frac{89.06}{24}}=61.4 \pm 4.97=$$
$$IC_{99\%}(\mu): [56.43 \quad 66.37]$$

Al 95%,

$$IC_{1-\alpha}(\mu): 61.4 \pm 1.96 *\sqrt{\frac{89.06}{24}}=61.4 \pm 3.78=$$

$$IC_{95\%}(\mu): [57.62 \quad 65.18]$$

Al 90%,

$$IC_{1-\alpha}(\mu): 61.4 \pm 1.64 *\sqrt{\frac{89.06}{24}}=61.4 \pm 3.16=$$

$$IC_{90\%}(\mu): [58.24 \quad 64.56]$$

Como un caso particular de la media se tiene a la proporción, esto ocurre cuando los datos son dicotómicos/binarios. Es decir:

$$\bar{x}=\hat{P}_a=\frac{\sum_s x_i}{n}=\frac{\#a}{n}$$

$$\hat{P} \sim N\left(P,\frac{\sigma_p^2}{n}\right)=N\left(P,\frac{P(1-P)}{n}\right)$$

$$\sigma^2_p=\frac{\sum_U x_i^2}{N}-\mu^2=\frac{\sum_U x_i}{N}-P^2=P-P^2=P(1-P)$$

La proporción mide la participación de una característica dentro de la población/muestra. Por ejemplo

  * Proporción de mujeres

$$P_{m}=\frac{14}{24}=0.58 \rightarrow 58\%$$  
  
  * Proporción de hombres

$$P_{h}=\frac{10}{24}=0.42 \rightarrow 42\%$$

De esta forma, el intervalo de confianza para la proporción queda de la forma:

$$IC_{1-\alpha}(P): \hat{P} \pm z_{\alpha/2} *\sqrt{\frac{\hat{P}(1-\hat{P})}{n}}$$

> Ejemplo: 

Se toma una muestra aleatoria de 35 cachorros y sobre este grupo se evidencia que 23 de ellos cuentan con sus vacunas respectivas. Calcule un **intervalo de confianza** al 95% para la *proporción de cachorros sin vacunas*. Si la muestra fue obtenida de un albergue de 200 cachorros, ¿cuál será el intervalo de confianza para el **total de cachorros** sin vacunas.?

Solución, como información $n=35$, $N=200$, 23 tienen vacunas.

$$\hat{P}_{sv}=\frac{12}{35}$$

$$IC_{1-\alpha}(P):\frac{12}{35} \pm 1.96 *\sqrt{\frac{\frac{12}{35}*\frac{23}{35}}{35}}=0.34 \pm 0.16$$

$$IC_{95\%}(P): [0.18 \quad 0.5] \rightarrow (\%)[18 \quad 50]$$

Una ventaja de la proporción es que se encuentra en unidades relativas, para expandir sus resultados a una población mayor bastara con multiplicar por el tamaño de la población.

$$IC_{95\%}(T=P*N): 200*([0.18 \quad 0.5]) \rightarrow [36 \quad 100]$$

> Ejercicio

Se realizó la toma de muestra de 45 palomas de la plaza Murillo un día cualquiera, se identificó a 31 palomas con alguna dificultad/dolencia en alguna de sus patas. 

  * Calcular el intervalo de confianza al 95% de confiabilidad para la proporción de palomas con problemas en alguna de sus patas. 
  * Si suponemos que la población de palomas alrededor de la plaza Murillo ronda las 7000 palomas, ¿cuál será el intervalo de confianza al 95% de confiabilidad del total de palomas sin problemas en sus patas?

Solución, $n=45$ 31 palomas con dificultades en sus patas, 14 palomas sin dificultades.

$$\hat{P}_{cd}=\frac{31}{45}=0.69; \quad \hat{P}_{sd}=\frac{14}{45}=0.31$$

$$IC_{1-\alpha}(P_{cd}): 0.69 \pm 1.96 *\sqrt{\frac{0.69*0.31}{45}}=0.69\pm 0.14=$$
$$IC_{95\%}(P_{cd}): [0.55 \quad 0.83]\rightarrow (\%)[55 \quad 83]$$

$$IC_{1-\alpha}(P_{sd}): 0.31 \pm 1.96 *\sqrt{\frac{0.69*0.31}{45}}=0.31\pm 0.14=$$

$$IC_{95\%}(T_{sd}=N*P_{sd}):7000( [0.17 \quad 0.45]) \rightarrow [1190 \quad 3150]$$

## pruebas de hipótesis

El principal objetivo de la **inferencia** estadística es aproximarse al valor del parámetro $\theta$ de la población ($U$), mediante un estimador $\hat{\theta}$ que viene definido por una muestra aleatoria ($s$).

Las estrategias de estimación vistas hasta ahora son:

  * Estimación puntual: 
  
$$\mu=\frac{\sum_U x_i}{N} \rightarrow \bar{x}=\frac{\sum_s x_i}{n}$$
  
  * Estimación por intervalo de confianza
  
$$IC_{1-\alpha}(\mu): \bar{x}\pm z_{\alpha/2}\sqrt{\frac{\hat{S}^2}{n}}$$

Ahora veremos las pruebas de hipótesis estadísticas, estas pruebas partes de una conjetura nuestra al rededor del parámetro de interés y esta conjetura es verificada mediante la información de la muestra. 

Cuando se elabora una hipótesis estadística se debe plantear 2 elementos; la hipótesis que se plantea (hipótesis nula) y el complemento de esta hipótesis planteada se denomina hipótesis alternativa. Ejemplo

$$H_0: \theta = k$$
$$H_1: \theta \neq k$$
$$H_1: \theta > k$$
$$H_1: \theta < k$$
Ejemplo, se observa a los estudiantes inscritos en la materia de estadística mediante una muestra de 10 estudiantes.  Si la variable de interés es la estatura en centímetros

$$H_0: \mu = 165$$
$$H_1: \mu< 165$$

La manera de verificar la hipótesis planteada pasara por estudiar una muestra aleatoria, sobre la cual se pueda establecer una regla que nos permita decidir si la hipótesis es correcta o no. Normalmente se inicia calculando un **estadístico de prueba** que nos permitirá decidir con base a una regla definida en una **región de aceptación**.

Al momento de tomar una decisión con base al estadístico de prueba y las regiones de aceptación y rechazo empleando la información de la muestra, es posible cometer errores dado el resultado real en $U$. En las pruebas de hipótesis existen dos tipos de errores, el **error de tipo I** y el **error de tipo II**:

  * Error de tipo I: ($\alpha$) también conocido como un falso positivo. Rechazar algo verdadero
  * Error de tipo II: ($\beta$) también conocido como falso negativo. Aceptando algo falso
  
### Prueba de hipótesis para la media 






